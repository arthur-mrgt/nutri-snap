{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "import torch\n",
    "from torchvision.transforms.functional import center_crop\n",
    "from tokenizers import Tokenizer\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Video related: \n",
    "import cv2\n",
    "from IPython.display import Video\n",
    "\n",
    "from fourm.data.multimodal_dataset_folder import MultiModalDatasetFolder\n",
    "from fourm.models.fm import FM\n",
    "from fourm.vq.vqvae import VQVAE, DiVAE\n",
    "from fourm.models.generate import GenerationSampler, build_chained_generation_schedules, init_empty_target_modality, init_full_input_modality, custom_text\n",
    "# from utils.generation_abstract_functions import create_generation_schedule_rgb_to_others\n",
    "from fourm.data.modality_transforms import RGBTransform, DepthTransform, MetadataTransform\n",
    "from fourm.data.modality_info import MODALITY_INFO, MODALITY_TRANSFORMS\n",
    "from fourm.utils.plotting_utils import decode_dict, visualize_bboxes, plot_text_in_square\n",
    "from fourm.utils import denormalize, IMAGENET_INCEPTION_MEAN, IMAGENET_INCEPTION_STD, IMAGENET_DEFAULT_STD, IMAGENET_DEFAULT_MEAN\n",
    "from fourm.data.modality_transforms import SemsegTransform\n",
    "from fourm.data.image_augmenter import CenterCropImageAugmenter\n",
    "from torchvision import transforms\n",
    "from fourm.data.modality_transforms import UnifiedDataTransform\n",
    "from fourm.data.dataset_utils import SubsampleDatasetWrapper\n",
    "from fourm.data.masking import UnifiedMasking\n",
    "from einops import rearrange\n",
    "from utils.semseg_helper_utils import semseg_to_rgb, plot_rgb2semseg, get_dataset, get_semseg_metrics, total_intersect_and_union, intersect_and_union, mean_iou, mean_dice, eval_metrics, tokens_per_target_dict, autoregression_schemes_dict, cfg_schedules_dict\n",
    "from tqdm import tqdm\n",
    "import matplotlib.colors as mcolors\n",
    "\n",
    "# The flag below controls whether to allow TF32 on matmul. This flag defaults to False in PyTorch 1.12 and later.\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "# The flag below controls whether to allow TF32 on cuDNN. This flag defaults to True.\n",
    "torch.backends.cudnn.allow_tf32 = True\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "torch.set_grad_enabled(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "text_tok = Tokenizer.from_file('toks/text_tokenizer_4m_wordpiece_30k.json')\n",
    "\n",
    "toks = {\n",
    "    'tok_rgb': DiVAE.from_pretrained('EPFL-VILAB/4M_tokenizers_rgb_16k_224-448').eval().to(device),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# initalizing the RGB transform class\n",
    "rgb_transform = RGBTransform(imagenet_default_mean_and_std=False)\n",
    "img_pil = rgb_transform.load('data/processed/train/rgb/dish_1558031526/dish_1558031526.png')\n",
    "img_pil = rgb_transform.preprocess(img_pil)\n",
    "img_pil = center_crop(img_pil, (min(img_pil.size), min(img_pil.size))).resize((224,224))\n",
    "img = rgb_transform.postprocess(img_pil).unsqueeze(0).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# 1. Définissez le chemin de votre fichier .npy\n",
    "npy_file_path = 'data/processed/train/rgb_tok/dish_1558031526/dish_1558031526.npy' # Ceci est une chaîne\n",
    "\n",
    "# 2. Chargez les données du fichier .npy en utilisant np.load()\n",
    "# Cela chargera les tokens sous forme d'un tableau NumPy.\n",
    "# En supposant que save_vq_tokens.py a sauvegardé les tokens avec une forme comme (n_crops, sequence_length)\n",
    "try:\n",
    "    all_crop_tokens_numpy = np.load(npy_file_path)\n",
    "    print(f\"Fichier NumPy chargé avec succès. Forme : {all_crop_tokens_numpy.shape}\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"ERREUR : Fichier non trouvé à {npy_file_path}\")\n",
    "    # Gérez l'erreur ici, par exemple en sortant du script ou en levant une exception\n",
    "    raise\n",
    "except Exception as e:\n",
    "    print(f\"Une erreur est survenue lors du chargement du fichier NumPy : {e}\")\n",
    "    raise\n",
    "\n",
    "# 3. Sélectionnez le crop désiré (si n_crops > 1)\n",
    "# Par exemple, pour le premier crop (center crop) :\n",
    "CROP_INDEX_TO_VISUALIZE = 0\n",
    "if CROP_INDEX_TO_VISUALIZE >= all_crop_tokens_numpy.shape[0]:\n",
    "    print(f\"ERREUR : L'index de crop {CROP_INDEX_TO_VISUALIZE} est hors limites pour {all_crop_tokens_numpy.shape[0]} crops.\")\n",
    "    raise IndexError(\"Index de crop invalide\")\n",
    "\n",
    "selected_tokens_numpy = all_crop_tokens_numpy[CROP_INDEX_TO_VISUALIZE] # Forme : (sequence_length,)\n",
    "print(f\"Tokens pour le crop {CROP_INDEX_TO_VISUALIZE} sélectionnés. Forme : {selected_tokens_numpy.shape}\")\n",
    "\n",
    "\n",
    "# 4. Convertissez le tableau NumPy en tenseur PyTorch\n",
    "# Les tokens VQVAE sont généralement des entiers (IDs).\n",
    "tokenized_rgb_tensor = torch.from_numpy(selected_tokens_numpy).long() # Convertit en torch.LongTensor\n",
    "\n",
    "# 5. Remodelez le tenseur pour qu'il ait la forme attendue par decode_tokens.\n",
    "# La méthode decode_tokens attend généralement (Batch, H_token_map, W_token_map).\n",
    "# Vous devez connaître H_tok et W_tok pour votre modèle.\n",
    "# Par exemple, si votre tokenizer produit une carte de tokens de 14x14 :\n",
    "H_tok = 14 # À remplacer par la hauteur réelle de votre carte de tokens\n",
    "W_tok = 14 # À remplacer par la largeur réelle de votre carte de tokens\n",
    "\n",
    "if tokenized_rgb_tensor.shape[0] != H_tok * W_tok:\n",
    "    print(f\"ERREUR : La longueur de la séquence de tokens ({tokenized_rgb_tensor.shape[0]}) \"\n",
    "          f\"ne correspond pas à H_tok*W_tok ({H_tok*W_tok}).\")\n",
    "    print(\"Veuillez vérifier les valeurs de H_tok et W_tok, ou la validité du fichier de tokens.\")\n",
    "    raise ValueError(\"Incompatibilité de forme des tokens.\")\n",
    "\n",
    "# Ajoutez une dimension de batch (B=1) et remodelez en (1, H_tok, W_tok)\n",
    "tokenized_rgb_prepared = tokenized_rgb_tensor.reshape(1, H_tok, W_tok)\n",
    "print(f\"Tokens préparés pour le décodage. Forme : {tokenized_rgb_prepared.shape}\")\n",
    "\n",
    "# Assurez-vous que le tenseur est sur le bon device (celui de votre modèle toks['tok_rgb'])\n",
    "# Supposons que votre modèle est sur 'cuda' s'il est disponible\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "tokenized_rgb_prepared = tokenized_rgb_prepared.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "tokenized_rgb_prepared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "reconstructed_rgb = toks['tok_rgb'].decode_tokens(tokenized_rgb_prepared, image_size=224, timesteps=19)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# reconstructed_rgb\n",
    "# Create a figure with two subplots (1 row, 2 columns)\n",
    "fig, axes = plt.subplots(1, 2, figsize=(10, 5))\n",
    "\n",
    "# Display the original image on the left\n",
    "axes[0].imshow(denormalize(img, mean=IMAGENET_INCEPTION_MEAN, std=IMAGENET_INCEPTION_STD)[0].permute(1, 2, 0).cpu())\n",
    "axes[0].set_title(\"Original Image\")\n",
    "axes[0].axis(\"off\")  # Hide axes\n",
    "\n",
    "# Display the reconstructed image on the right\n",
    "axes[1].imshow(denormalize(reconstructed_rgb, mean=IMAGENET_INCEPTION_STD, std=IMAGENET_INCEPTION_STD)[0].permute(1, 2, 0).cpu())\n",
    "axes[1].set_title(\"Reconstructed Image\")\n",
    "axes[1].axis(\"off\")  # Hide axes\n",
    "\n",
    "# Show the figure\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
