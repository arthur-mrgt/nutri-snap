{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "import torch\n",
    "from torchvision.transforms.functional import center_crop\n",
    "from tokenizers import Tokenizer\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Video related: \n",
    "import cv2\n",
    "from IPython.display import Video\n",
    "\n",
    "from fourm.data.multimodal_dataset_folder import MultiModalDatasetFolder\n",
    "from fourm.models.fm import FM\n",
    "from fourm.vq.vqvae import VQVAE, DiVAE\n",
    "from fourm.models.generate import GenerationSampler, build_chained_generation_schedules, init_empty_target_modality, init_full_input_modality, custom_text\n",
    "# from utils.generation_abstract_functions import create_generation_schedule_rgb_to_others\n",
    "from fourm.data.modality_transforms import RGBTransform, DepthTransform, MetadataTransform\n",
    "from fourm.data.modality_info import MODALITY_INFO, MODALITY_TRANSFORMS\n",
    "from fourm.utils.plotting_utils import decode_dict, visualize_bboxes, plot_text_in_square\n",
    "from fourm.utils import denormalize, IMAGENET_INCEPTION_MEAN, IMAGENET_INCEPTION_STD, IMAGENET_DEFAULT_STD, IMAGENET_DEFAULT_MEAN\n",
    "from fourm.data.modality_transforms import SemsegTransform\n",
    "from fourm.data.image_augmenter import CenterCropImageAugmenter\n",
    "from torchvision import transforms\n",
    "from fourm.data.modality_transforms import UnifiedDataTransform\n",
    "from fourm.data.dataset_utils import SubsampleDatasetWrapper\n",
    "from fourm.data.masking import UnifiedMasking\n",
    "from einops import rearrange\n",
    "from utils.semseg_helper_utils import semseg_to_rgb, plot_rgb2semseg, get_dataset, get_semseg_metrics, total_intersect_and_union, intersect_and_union, mean_iou, mean_dice, eval_metrics, tokens_per_target_dict, autoregression_schemes_dict, cfg_schedules_dict\n",
    "from tqdm import tqdm\n",
    "import matplotlib.colors as mcolors\n",
    "\n",
    "# The flag below controls whether to allow TF32 on matmul. This flag defaults to False in PyTorch 1.12 and later.\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "# The flag below controls whether to allow TF32 on cuDNN. This flag defaults to True.\n",
    "torch.backends.cudnn.allow_tf32 = True\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "torch.set_grad_enabled(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "text_tok = Tokenizer.from_file('toks/text_tokenizer_4m_wordpiece_30k.json')\n",
    "\n",
    "toks = {\n",
    "    'tok_rgb': DiVAE.from_pretrained('EPFL-VILAB/4M_tokenizers_rgb_16k_224-448').eval().to(device),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# initalizing the RGB transform class\n",
    "input.jpg = 'nutri-snap/data/processed/train/rgb/dish_1558031526/dish_1558031526.png'\n",
    "rgb_transform = RGBTransform(imagenet_default_mean_and_std=False)\n",
    "img_pil = rgb_transform.load('./input.jpg')\n",
    "img_pil = rgb_transform.preprocess(img_pil)\n",
    "img_pil = center_crop(img_pil, (min(img_pil.size), min(img_pil.size))).resize((224,224))\n",
    "img = rgb_transform.postprocess(img_pil).unsqueeze(0).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "tokenized_rgb = 'nutri-snap/data/processed/train/rgb_tok/dish_1558031526/dish_1558031526.npy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "reconstructed_rgb = toks['tok_rgb'].decode_tokens(tokenized_rgb, image_size=224, timesteps=19)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# reconstructed_rgb\n",
    "# Create a figure with two subplots (1 row, 2 columns)\n",
    "fig, axes = plt.subplots(1, 2, figsize=(10, 5))\n",
    "\n",
    "# Display the original image on the left\n",
    "axes[0].imshow(denormalize(img, mean=IMAGENET_INCEPTION_MEAN, std=IMAGENET_INCEPTION_STD)[0].permute(1, 2, 0).cpu())\n",
    "axes[0].set_title(\"Original Image\")\n",
    "axes[0].axis(\"off\")  # Hide axes\n",
    "\n",
    "# Display the reconstructed image on the right\n",
    "axes[1].imshow(denormalize(reconstructed_rgb, mean=IMAGENET_INCEPTION_STD, std=IMAGENET_INCEPTION_STD)[0].permute(1, 2, 0).cpu())\n",
    "axes[1].set_title(\"Reconstructed Image\")\n",
    "axes[1].axis(\"off\")  # Hide axes\n",
    "\n",
    "# Show the figure\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
