# nutrisnap_generation_model.yaml

# Base model for generation
model_base_id: '/work/com-304/nutri-snap/output/nutrisnap_finetune/checkpoint-final.pth' # IMPORTANT: Path to your fine-tuned model checkpoint (e.g., from output_dir of training)
model_base_load_from_hf: False # Set to True if loading from Hugging Face Hub, False for local checkpoint

# Tokenizer definitions
# Ensure these names (rgb, depth, semseg, caption) and their 'domain' configs
# match the modalities your model was trained on and expects.

tokenizers:
  rgb: # Tokenizer for the input tok_rgb@224. Used for loading/passing the input.
    tokenizer_id: 'EPFL-VILAB/4M_tokenizers_rgb_16k_224-448' # Or the path to your converted .pth for rgb_vq_tokens_for_224
    tokenizer_root: '/work/com-304/nutri-snap/external_libs/ml_4m/tokenizer_ckpts'
    config:
      target_size: 224
      domain: 'tok_rgb@224' # This is the input conditioning modality

  depth:
    tokenizer_id: 'EPFL-VILAB/4M_tokenizers_depth_8k_224-448' # IMPORTANT: Specify your depth tokenizer ID or path (e.g., HF ID or path to .pth)
    tokenizer_root: '/work/com-304/nutri-snap/external_libs/ml_4m/tokenizer_ckpts'
    config:
      domain: 'tok_depth@224' # Internal modality name for depth tokens, must match training

  semseg:
    tokenizer_id: '/work/com-304/nutri-snap/output/semseg_n5k_tokenizer_ViTB_8k_224/checkpoint-35.pth' # Path to your custom-trained semseg tokenizer checkpoint
    tokenizer_root: '' # Empty as tokenizer_id is a full path
    config:
      domain: 'tok_semseg_n5k@224' # Internal modality name for segmentation tokens, must match training

  caption:
    tokenizer_id: 'external_libs/ml_4m/fourm/utils/tokenizer/trained/text_tokenizer_4m_wordpiece_30k.json' # Path to the text tokenizer used during training
    tokenizer_root: '' # Can be empty if tokenizer_id is a full path or HF ID not needing a root
    config:
      domain: 'caption' # Internal modality name for captions, must match training

# Super-resolution model configuration (if used, otherwise can be removed or commented out)
# model_sr_id: ...
# tokenizers_sr: ...